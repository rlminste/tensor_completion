
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Poblano Toolbox for MATLAB: Overview</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2019-04-25"><meta name="DC.source" content="overview_doc.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }

.background {
	background-color:#004889;
	background-image: url(images/banner-background.jpg);
	background-repeat:no-repeat;
}

  </style></head><body><div class="background"><a href="index.html"><img area="13494" src="images/logo.gif" alt="Sandia National Laboratories" border="0" height="78" width="173"></a></div><div class="content"><h1>Poblano Toolbox for MATLAB: Overview</h1><!--introduction--><p>Poblano is a toolbox of large-scale algorithms for nonlinear optimization. The algorithms in Poblano require only first-order derivative information (e.g., gradients for scalar-valued objective functions).</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#2">Introduction</a></li><li><a href="#8">Optimization Methods</a></li><li><a href="#10">Globalization Strategies</a></li><li><a href="#12">Optimization Input Parameters</a></li><li><a href="#14">Optimization Output Parameters</a></li><li><a href="#16">Checking Gradient Calculations</a></li><li><a href="#18">Examples</a></li><li><a href="#20">Calling a Poblano Optimizer</a></li><li><a href="#25">Acknowledgments</a></li><li><a href="#27">References</a></li></ul></div><p><hr></p><h2 id="2">Introduction</h2><p>Poblano optimizers find local minimizers of scalar-valued objective functions taking vector inputs. Specifically, the problems solved by Poblano optimizers are of the following form:</p><p><img src="overview_doc_eq00884557479967417095.png" alt="$$\min_{x}f(x), \quad \mbox{where} \quad f:R^n \rightarrow R$$" style="width:76px;height:8px;"></p><p>The gradient of the objective function, <img src="overview_doc_eq07500982203449410655.png" alt="$\nabla f(x)$" style="width:15px;height:6px;">, is required for all Poblano optimizers. The optimizers converge to a stationary point, <img src="overview_doc_eq08097522164358817053.png" alt="$x^*$" style="width:5px;height:4px;">, where</p><p><img src="overview_doc_eq16485556778937288589.png" alt="$$\nabla f(x^*) \approx 0$$" style="width:28px;height:6px;"></p><p>A line search satisfying the strong Wolfe conditions is used to guarantee global convergence of the Poblano optimizers.</p><p><hr></p><h2 id="8">Optimization Methods</h2><p>The following optimization methods are available in Poblano.</p><p><b>Nonlinear conjugate gradient method</b> (<tt>ncg</tt>) [4]</p><div><ul><li>Uses Fletcher-Reeves, Polak-Ribiere, and Hestenes-Stiefel conjugate direction updates</li><li>Restart strategies based on number of iterations or orthogonality of gradients across iterations</li><li>Steepest descent method is a special case</li></ul></div><p><b>Limited-memory BFGS method</b> (<tt>lbfgs</tt>) [4]</p><div><ul><li>Uses a two-loop recursion for approximate Hessian-gradient products</li></ul></div><p><b>Truncated Newton method</b> (<tt>tn</tt>) [1]</p><div><ul><li>Uses finite differencing for approximate Hessian-vector products</li></ul></div><p><hr></p><h2 id="10">Globalization Strategies</h2><p><b>Line search methods</b></p><div><ul><li>More-Thuente cubic interpolation line search (<tt>cvsrch</tt>) [3]</li></ul></div><p><hr></p><h2 id="12">Optimization Input Parameters</h2><p>Input parameters are passed to the different optimization methods using Matlab <tt>inputParser</tt> objects. Some parameters are shared across all methods and others are specific to a particular method. Below are descriptions of the shared input parameters and examples of how to set and use these parameters in the optimization methods. The Poblano function <tt>poblano_params</tt> is used by the optimization methods to set the input parameters. See the <a href="A2_poblano_params_docs.html">Optimization Input Parameters</a> documentation for more details.</p><p><hr></p><h2 id="14">Optimization Output Parameters</h2><p>Each of the optimization methods in Poblano outputs a single structure containing fields for the approximate solution, function and gradient values at the solution, and various information about the optimization run (e.g., number of function evaluations, etc.). The Poblano function <tt>poblano_out</tt> is used by the optimization methods to set the output parameters. See the <a href="A3_poblano_out_docs.html">Optimization Output Parameters</a> documentation for more details.</p><p><hr></p><h2 id="16">Checking Gradient Calculations</h2><p>Analytic gradients can be checked using finite difference approximations. The Poblano function <tt>gradientcheck</tt> computes the gradient approximations and compares the results to the analytic gradient using a user-supplied objective function/gradient M-file. The user can choose one of several difference formulas as well as the difference step used in the computations. See the <a href="E_gradientcheck_docs.html">Checking Gradient Calculations</a> documentation for more details.</p><p><hr></p><h2 id="18">Examples</h2><p>Poblano provides two example function/gradient M-files:</p><div><ul><li><tt>example1</tt>: simple multivariate function</li><li><tt>example2</tt>: more complicated function of a matrix variable</li></ul></div><p>See the <a href="A4_poblano_examples_docs.html">Poblano Examples</a> documentation for more details.</p><p><hr></p><h2 id="20">Calling a Poblano Optimizer</h2><p>All Poblano methods are called using the name of the method along with two required arguments and one or more optional arguments. The required arguments are 1) a handle to the function being minimized, and 2) the initial guess of the solution (as a scalar or column vector). For example, the following is a call to the <tt>ncg</tt> method to minimize the <tt>example1</tt> function distributed with Poblano starting with an initial guess of <img src="overview_doc_eq13721435717236182146.png" alt="$x = \pi/4$" style="width:20px;height:6px;"> and using the default <tt>ncg</tt> parameters.</p><pre class="codeinput">ncg(@example1, pi/4);
</pre><pre class="codeoutput"> Iter  FuncEvals       F(X)          ||G(X)||/N        
------ --------- ---------------- ----------------
     0         1       0.70710678       0.70710678
     1         6      -0.99999998       0.00017407
     2         7      -1.00000000       0.00000000
</pre><p>Parameterize functions can be optimized using Poblano as well. For such functions, the function handle can be used to specify the function parameters. For example, Poblano's <tt>example1</tt> function takes an optional scalar parameter as follows.</p><pre class="codeinput">ncg(@(x) example1(x,3), pi/4);
</pre><pre class="codeoutput"> Iter  FuncEvals       F(X)          ||G(X)||/N        
------ --------- ---------------- ----------------
     0         1       0.70710678       2.12132034
     1        14      -0.99998885       0.01416497
     2        16      -1.00000000       0.00000147
</pre><p>Functions taking vectors as inputs can be optimized using Poblano as well. For functions which can take input vectors of arbitrary sizes (e.g., Matlab functions such as <tt>sin</tt>, <tt>fft</tt>, etc.), the size of the initial guess (as a scalar or column vector) determines the size of the problem to be solved. For example, Poblano's \texttt{example1} function can take as input a vector (in this case a vector in <img src="overview_doc_eq13754963546198947976.png" alt="$R^3$" style="width:6px;height:5px;">) as follows.</p><pre class="codeinput">ncg(@(x) example1(x,3), [pi/5 pi/4 pi/3]');
</pre><pre class="codeoutput"> Iter  FuncEvals       F(X)          ||G(X)||/N        
------ --------- ---------------- ----------------
     0         1       1.65816330       1.26312767
     1         5      -0.94404964       0.49911677
     2         9      -1.64717576       1.35314598
     3        14      -1.82513979       1.19534779
     4        19      -2.96660154       0.25769184
     5        22      -2.99851295       0.05452749
     6        24      -2.99999996       0.00028818
     7        26      -3.00000000       0.00000000
</pre><p>The optional arguments are input parameters specifying how the optimization method is to be run.  See the <a href="A2_poblano_params_docs.html">Input Parameters</a> documentation for details about the input parameters.)</p><p><hr></p><h2 id="25">Acknowledgments</h2><div><ul><li>Dianne O'Leary of the University of Maryland, College Park, provided the Matlab translation of the MINPACK implementation of the More-Thuente line search.</li><li>This product includes software developed by the University of Chicago, as Operator of Argonne National Laboratory.</li></ul></div><p><hr></p><h2 id="27">References</h2><p>[1] Dembo, R.S. and and Steihaug, T. (1983). Truncated-Newton algorithms for large-scale unconstrained minimization. <i>Mathematical Programming</i>, 26, 190-212.</p><p>[2] Golub, G. H. and Loan, C. F. V. (1996). <i>Matrix Computations</i>. Johns Hopkins University Press.</p><p>[3] More, J. J. and Thuente, D. J. (1994). Line search algorithms with guaranteed sufficient decrease. <i>ACM Trans. Math. Softw.</i>, 20, 286-307.</p><p>[4] Nocedal, J. and Wright S. J. (1999). <i>Numerical Optimization</i>. Springer.</p><p class="footer"><br>
      Copyright 2019, National Technology &amp; Engineering Solutions of Sandia, LLC (NTESS).<br></p></div><!--
##### SOURCE BEGIN #####
%% Poblano Toolbox for MATLAB: Overview
% Poblano is a toolbox of large-scale algorithms for nonlinear
% optimization. The algorithms in Poblano require only first-order
% derivative information (e.g., gradients for scalar-valued objective
% functions).
%%
%
% <html><hr></html>
%% Introduction
% Poblano optimizers find local minimizers of scalar-valued objective
% functions taking vector inputs. Specifically, the problems solved by
% Poblano optimizers are of the following form:
%%
% 
% $$\min_{x}f(x), \quad \mbox{where} \quad f:R^n \rightarrow R$$
%%
% 
% The gradient of the objective function, $\nabla f(x)$, is required
% for all Poblano optimizers. The optimizers converge to a stationary point, 
% $x^*$, where
%%
%
% $$\nabla f(x^*) \approx 0$$
%%
% A line search satisfying the strong Wolfe conditions is used to 
% guarantee global convergence of the Poblano optimizers.
%%
%
% <html><hr></html>
%% Optimization Methods
% The following optimization methods are available in Poblano.
% 
% *Nonlinear conjugate gradient method* (|ncg|) [4]
%
% * Uses Fletcher-Reeves, Polak-Ribiere, and Hestenes-Stiefel conjugate direction updates 
% * Restart strategies based on number of iterations or orthogonality of gradients across iterations 
% * Steepest descent method is a special case 
%
% *Limited-memory BFGS method* (|lbfgs|) [4]
%
% * Uses a two-loop recursion for approximate Hessian-gradient products
%
% *Truncated Newton method* (|tn|) [1]
%
% * Uses finite differencing for approximate Hessian-vector products
%
%%
%
% <html><hr></html>
%% Globalization Strategies
%
% *Line search methods*
%
% * More-Thuente cubic interpolation line search (|cvsrch|) [3]
%%
%
% <html><hr></html>
%% Optimization Input Parameters
%
% Input parameters are passed to the different optimization methods using
% Matlab |inputParser| objects. Some parameters are shared across all
% methods and others are specific to a particular method. Below are
% descriptions of the shared input parameters and examples of how to set
% and use these parameters in the optimization methods. The Poblano
% function |poblano_params| is used by the optimization methods to set the
% input parameters. See the <A2_poblano_params_docs.html Optimization Input
% Parameters> documentation for more details.
%%
%
% <html><hr></html>
%% Optimization Output Parameters
%
% Each of the optimization methods in Poblano outputs a single structure
% containing fields for the approximate solution, function and gradient
% values at the solution, and various information about the optimization
% run (e.g., number of function evaluations, etc.). The Poblano function
% |poblano_out| is used by the optimization methods to set the output
% parameters. See the <A3_poblano_out_docs.html Optimization Output
% Parameters> documentation for more details.
%%
%
% <html><hr></html>

%% Checking Gradient Calculations
%
% Analytic gradients can be checked using finite difference approximations.
% The Poblano function |gradientcheck| computes the gradient approximations
% and compares the results to the analytic gradient using a user-supplied
% objective function/gradient M-file. The user can choose one of several
% difference formulas as well as the difference step used in the
% computations. See the <E_gradientcheck_docs.html Checking Gradient
% Calculations> documentation for more details.
%%
%
% <html><hr></html>
%% Examples
%
% Poblano provides two example function/gradient M-files:
% 
% * |example1|: simple multivariate function
% * |example2|: more complicated function of a matrix variable
%
% See the <A4_poblano_examples_docs.html Poblano Examples> documentation
% for more details.
%%
%
% <html><hr></html>
%% Calling a Poblano Optimizer
% All Poblano methods are called using the name of the method along with
% two required arguments and one or more optional arguments. The required
% arguments are 1) a handle to the function being minimized, and 2) the
% initial guess of the solution (as a scalar or column vector). For
% example, the following is a call to the |ncg| method to minimize the
% |example1| function distributed with Poblano starting with an initial
% guess of $x = \pi/4$ and using the default |ncg| parameters.
ncg(@example1, pi/4);
%%
% Parameterize functions can be optimized using Poblano as well. For such
% functions, the function handle can be used to specify the function
% parameters. For example, Poblano's |example1| function takes an optional
% scalar parameter as follows.
ncg(@(x) example1(x,3), pi/4);
%%
% Functions taking vectors as inputs can be optimized using Poblano as
% well. For functions which can take input vectors of arbitrary sizes
% (e.g., Matlab functions such as |sin|, |fft|, etc.), the
% size of the initial guess (as a scalar or column vector) determines the
% size of the problem to be solved. For example, Poblano's
% \texttt{example1} function can take as input a vector (in this case a
% vector in $R^3$) as follows.
ncg(@(x) example1(x,3), [pi/5 pi/4 pi/3]');
%% 
% The optional arguments are input parameters specifying how the
% optimization method is to be run.  See the <A2_poblano_params_docs.html
% Input Parameters> documentation for details about the input parameters.)
%%
%
% <html><hr></html>
%% Acknowledgments
%
% * Dianne O'Leary of the University of Maryland, College Park, provided
% the Matlab translation of the MINPACK implementation of the More-Thuente
% line search.
% * This product includes software developed by the University of Chicago,
% as Operator of Argonne National Laboratory.
%
%%
%
% <html><hr></html>
%% References
%
% [1] Dembo, R.S. and and Steihaug, T. (1983). Truncated-Newton algorithms 
% for large-scale unconstrained minimization. _Mathematical Programming_, 
% 26, 190-212.
%
% [2] Golub, G. H. and Loan, C. F. V. (1996). _Matrix Computations_. 
% Johns Hopkins University Press.
%
% [3] More, J. J. and Thuente, D. J. (1994). Line search algorithms with 
% guaranteed sufficient decrease. _ACM Trans. Math. Softw._, 20, 286-307.
%
% [4] Nocedal, J. and Wright S. J. (1999). 
% _Numerical Optimization_. Springer.
%

##### SOURCE END #####
--></body></html>